{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe529bd5-7108-4912-bad1-1f4af8151331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "from unidecode import unidecode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676a40b3-6b86-4aef-a921-a753712973c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/95/csh3_2vs4nb11f7sndt720_w0000gn/T/ipykernel_4278/945850662.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books_df = pd.read_csv(\"Books.csv\", encoding='latin1')\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "books_df = pd.read_csv(\"Books.csv\", encoding='latin1')\n",
    "ratings_df = pd.read_csv(\"Ratings.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d212a6-d381-4ddd-bda7-be245a1fe150",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv(\"Users.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aab435a6-0772-40d3-9055-e611e4b16866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 271360 entries, 0 to 271359\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   ISBN                 271360 non-null  object\n",
      " 1   Book-Title           271360 non-null  object\n",
      " 2   Book-Author          271358 non-null  object\n",
      " 3   Year-Of-Publication  271360 non-null  object\n",
      " 4   Publisher            271358 non-null  object\n",
      " 5   Image-URL-S          271360 non-null  object\n",
      " 6   Image-URL-M          271360 non-null  object\n",
      " 7   Image-URL-L          271357 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 16.6+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1149780 entries, 0 to 1149779\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   User-ID      1149780 non-null  int64 \n",
      " 1   ISBN         1149780 non-null  object\n",
      " 2   Book-Rating  1149780 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 26.3+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 278858 entries, 0 to 278857\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   User-ID   278858 non-null  int64  \n",
      " 1   Location  278858 non-null  object \n",
      " 2   Age       168096 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 6.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display basic info about each dataset\n",
    "books_df.info()\n",
    "ratings_df.info()\n",
    "users_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b242617-8cc1-4669-afce-9165fed11792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp;amp; Company</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                         Book-Title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "\n",
       "            Book-Author Year-Of-Publication                   Publisher  \\\n",
       "0    Mark P. O. Morford                2002     Oxford University Press   \n",
       "1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
       "2          Carlo D'Este                1991             HarperPerennial   \n",
       "3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
       "4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
       "\n",
       "                                         Image-URL-S  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-M  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-L  \n",
       "0  http://images.amazon.com/images/P/0195153448.0...  \n",
       "1  http://images.amazon.com/images/P/0002005018.0...  \n",
       "2  http://images.amazon.com/images/P/0060973129.0...  \n",
       "3  http://images.amazon.com/images/P/0374157065.0...  \n",
       "4  http://images.amazon.com/images/P/0393045218.0...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the first few rows the dataset\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c44816a-5658-40ed-99bf-6f01c18847b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the image URL columns from the books dataset\n",
    "books_df = books_df.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c26647-a50e-4730-9cb4-aa89a5870ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp;amp; Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                         Book-Title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "\n",
       "            Book-Author Year-Of-Publication                   Publisher  \n",
       "0    Mark P. O. Morford                2002     Oxford University Press  \n",
       "1  Richard Bruce Wright                2001       HarperFlamingo Canada  \n",
       "2          Carlo D'Este                1991             HarperPerennial  \n",
       "3      Gina Bari Kolata                1999        Farrar Straus Giroux  \n",
       "4       E. J. W. Barber                1999  W. W. Norton &amp; Company  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the result\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f216ea3-e457-44c5-82b6-ffaa57201324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0195153448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>034542252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0771025661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1881320189</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1575663937</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID        ISBN  Book-Rating\n",
       "0        2  0195153448            0\n",
       "1        7   034542252            0\n",
       "2        8  0771025661            0\n",
       "3        8  1881320189            7\n",
       "4        8  1575663937            6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df = ratings_df.sort_values(by='User-ID', ascending=True)\n",
    "ratings_df = ratings_df.reset_index(drop=True)\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df7e8c2a-ae68-4ab2-a9f7-02532a5201d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID                            Location   Age\n",
       "0        1                  nyc, new york, usa   NaN\n",
       "1        2           stockton, california, usa  18.0\n",
       "2        3     moscow, yukon territory, russia   NaN\n",
       "3        4           porto, v.n.gaia, portugal  17.0\n",
       "4        5  farnborough, hants, united kingdom   NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42fcaea5-13d0-456d-b141-fce589e1a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df[['City', 'State', 'Country']] = users_df['Location'].str.split(',', n=2, expand=True)\n",
    "users_df = users_df.drop(columns='Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "705f6767-6b7f-4abc-b753-a1a46ad086a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only users who appear in the Ratings dataset\n",
    "valid_user_ids = ratings_df['User-ID'].unique()\n",
    "users_df = users_df[users_df['User-ID'].isin(valid_user_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f64ddef-34bf-41fb-9241-1f1e0e7d3ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unrealistic ages\n",
    "users_df = users_df[(users_df['Age'] >= 5) & (users_df['Age'] <= 100)]\n",
    "\n",
    "# (Optional) Impute missing ages with median\n",
    "users_df['Age'] = users_df['Age'].fillna(users_df['Age'].median())\n",
    "users_df = users_df.dropna(subset=['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9537a3-cbe5-420d-93b1-7150b38f358c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>stockton</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>albacete</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>weston</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>langhorne</td>\n",
       "      <td>pennsylvania</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>montreal</td>\n",
       "      <td>quebec</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID  Age       City          State  Country\n",
       "0        2   18   stockton     california      usa\n",
       "1       10   26   albacete      wisconsin    spain\n",
       "2       19   14     weston                        \n",
       "3       20   19  langhorne   pennsylvania      usa\n",
       "4       36   24   montreal         quebec   canada"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df['Age'] = users_df['Age'].astype(int)\n",
    "users_df = users_df.reset_index(drop=True)\n",
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ff63fb-021a-44f9-b7f0-3af43e2947d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df[['City', 'State', 'Country']] = users_df[['City', 'State', 'Country']].replace(r'^\\s*$', pd.NA, regex=True)\n",
    "users_df = users_df.dropna(subset=['City', 'State', 'Country'])\n",
    "users_df = users_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "037bd160-9d37-4fbb-8be2-cb2fa01c12e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City       0\n",
       "State      0\n",
       "Country    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df[['City', 'State', 'Country']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "098693e5-93e7-433c-80fc-7fbfb186e4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>stockton</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>albacete</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>langhorne</td>\n",
       "      <td>pennsylvania</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>montreal</td>\n",
       "      <td>quebec</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "      <td>viterbo</td>\n",
       "      <td>lazio</td>\n",
       "      <td>italy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID  Age       City          State  Country\n",
       "0        2   18   stockton     california      usa\n",
       "1       10   26   albacete      wisconsin    spain\n",
       "2       20   19  langhorne   pennsylvania      usa\n",
       "3       36   24   montreal         quebec   canada\n",
       "4       38   34    viterbo          lazio    italy"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea79da0d-dc41-4283-b2a1-468173a2f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numeric, set invalid entries to NaN\n",
    "books_df['Year-Of-Publication'] = pd.to_numeric(books_df['Year-Of-Publication'], errors='coerce')\n",
    "\n",
    "# Set anything outside 1700–2025 to NaN\n",
    "books_df.loc[\n",
    "    (books_df['Year-Of-Publication'] < 1700) | \n",
    "    (books_df['Year-Of-Publication'] > 2025),\n",
    "    'Year-Of-Publication'\n",
    "] = pd.NA\n",
    "\n",
    "books_df = books_df.dropna(subset=['Year-Of-Publication'])\n",
    "books_df = books_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3a4c787-1d06-4261-aa1f-470084798ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df['Year-Of-Publication'] = books_df['Year-Of-Publication'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed8e9649-3cc1-4f3e-ab22-c2fec14116ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only books that have at least one rating\n",
    "books_df = books_df[books_df['ISBN'].isin(ratings_df['ISBN'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92420518-a1d0-42f2-ba2c-2126cd5d5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_non_ascii(text):\n",
    "    return bool(re.search(r'[^\\x00-\\x7F]', str(text)))\n",
    "\n",
    "# Add helper flags to check for encoding issues\n",
    "books_df['title_corrupt'] = books_df['Book-Title'].apply(has_non_ascii)\n",
    "books_df['author_corrupt'] = books_df['Book-Author'].apply(has_non_ascii)\n",
    "books_df['publisher_corrupt'] = books_df['Publisher'].apply(has_non_ascii)\n",
    "\n",
    "books_df['short_title'] = books_df['Book-Title'].str.len() <= 2\n",
    "books_df['short_author'] = books_df['Book-Author'].str.len() <= 2\n",
    "books_df['short_publisher'] = books_df['Publisher'].str.len() <= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a77c2d7b-c3c6-4709-bf17-ebea9031e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 265537\n",
      "After: 264637\n",
      "Dropped: 900\n"
     ]
    }
   ],
   "source": [
    "books_df_cleaned = books_df[\n",
    "    ~(\n",
    "        ((books_df['short_title'] & books_df['title_corrupt']) |\n",
    "         (books_df['short_author'] & books_df['author_corrupt']) |\n",
    "         ((books_df['short_publisher'] | books_df['publisher_corrupt']) &\n",
    "          (books_df['title_corrupt'] | books_df['author_corrupt'])))  # Only drop if publisher is bad AND others too\n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(\"Before:\", len(books_df))\n",
    "print(\"After:\", len(books_df_cleaned))\n",
    "print(\"Dropped:\", len(books_df) - len(books_df_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8056769e-c1ad-4caa-94f7-d02d352074d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_cleaned = books_df_cleaned.drop(\n",
    "    columns=[\n",
    "        'title_corrupt', 'author_corrupt', 'publisher_corrupt',\n",
    "        'short_title', 'short_author', 'short_publisher'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a662674d-d25f-49ef-853d-f48f5df75adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_cleaned = books_df_cleaned.dropna(subset=['Book-Title', 'Book-Author', 'Publisher'])\n",
    "books_df_cleaned = books_df_cleaned.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08427072-f7c1-4482-b83f-68a254d5b661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp;amp; Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                         Book-Title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "\n",
       "            Book-Author  Year-Of-Publication                   Publisher  \n",
       "0    Mark P. O. Morford                 2002     Oxford University Press  \n",
       "1  Richard Bruce Wright                 2001       HarperFlamingo Canada  \n",
       "2          Carlo D'Este                 1991             HarperPerennial  \n",
       "3      Gina Bari Kolata                 1999        Farrar Straus Giroux  \n",
       "4       E. J. W. Barber                 1999  W. W. Norton &amp; Company  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ab13eb4-8810-43e7-b8ae-066512e40435",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_cleaned['Publisher'] = books_df_cleaned['Publisher'].apply(html.unescape)\n",
    "books_df_cleaned['Book-Title'] = books_df_cleaned['Book-Title'].apply(html.unescape)\n",
    "books_df_cleaned['Book-Author'] = books_df_cleaned['Book-Author'].apply(html.unescape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d893f509-0600-484f-b14d-33600aabc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_camel_case(text):\n",
    "    return re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)\n",
    "\n",
    "# Apply to Publisher column\n",
    "books_df_cleaned['Publisher'] = books_df_cleaned['Publisher'].apply(split_camel_case)\n",
    "books_df_cleaned['Book-Author'] = books_df_cleaned['Book-Author'].apply(split_camel_case)\n",
    "books_df_cleaned['Book-Title'] = books_df_cleaned['Book-Title'].apply(split_camel_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c122663a-5545-4e40-bdb6-552c89c89870",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_cleaned['Book-Title'] = books_df_cleaned['Book-Title'].str.title()\n",
    "books_df_cleaned['Book-Author'] = books_df_cleaned['Book-Author'].str.title()\n",
    "books_df_cleaned['Publisher'] = books_df_cleaned['Publisher'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab7b7f05-d13c-4097-84c6-5a4a3b355aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANON = {\n",
    "    # W. W. Norton\n",
    "    \"w w norton & co\": \"W. W. Norton & Company\",\n",
    "    \"w. w. norton & co\": \"W. W. Norton & Company\",\n",
    "    \"w w norton & company\": \"W. W. Norton & Company\",\n",
    "    \"w w norton & co inc\": \"W. W. Norton & Company\",\n",
    "    \"w. w. norton & company\": \"W. W. Norton & Company\",\n",
    "\n",
    "    # Little, Brown\n",
    "    \"little brown\": \"Little, Brown And Company\",\n",
    "    \"little brown & co\": \"Little, Brown And Company\",\n",
    "    \"little brown & company\": \"Little, Brown And Company\",\n",
    "    \"little brown and company\": \"Little, Brown And Company\",\n",
    "    \"little, brown\": \"Little, Brown And Company\",\n",
    "    \"little, brown & company\": \"Little, Brown And Company\",\n",
    "\n",
    "    # Knopf\n",
    "    \"knopf\": \"Alfred A. Knopf\",\n",
    "    \"a. a. knopf\": \"Alfred A. Knopf\",\n",
    "    \"alfred a. knopf\": \"Alfred A. Knopf\",\n",
    "\n",
    "    # Random House\n",
    "    \"random house inc\": \"Random House\",\n",
    "    \"random house, inc.\": \"Random House\",\n",
    "    \"random house canada\": \"Random House\",\n",
    "    \"random house value publishing\": \"Random House\",\n",
    "    \"random house\": \"Random House\",\n",
    "\n",
    "    # HarperCollins\n",
    "    \"harpercollins\": \"HarperCollins\",\n",
    "    \"harper collins\": \"HarperCollins\",\n",
    "    \"harper collins - uk\": \"HarperCollins\",\n",
    "    \"harper collins publishers\": \"HarperCollins\",\n",
    "    \"harpercollins publisher\": \"HarperCollins\",\n",
    "    \"harpercollins juvenile books\": \"HarperCollins\",\n",
    "    \"harpercollins library\": \"HarperCollins\",\n",
    "\n",
    "    # Penguin / Putnam\n",
    "    \"penguin usa\": \"Penguin Books\",\n",
    "    \"penguin usa (paper)\": \"Penguin Books\",\n",
    "    \"penguin books ltd\": \"Penguin Books\",\n",
    "    \"penguin books\": \"Penguin Books\",\n",
    "    \"penguin putnam~trade\": \"Penguin Putnam\",\n",
    "    \"penguin putnam\": \"Penguin Putnam\",\n",
    "\n",
    "    # Simon & Schuster\n",
    "    \"simon & schuster (trade division)\": \"Simon & Schuster\",\n",
    "    \"simon schuster trade\": \"Simon & Schuster\",\n",
    "    \"simon pulse\": \"Simon & Schuster\",\n",
    "    \"simon & schuster\": \"Simon & Schuster\",\n",
    "\n",
    "    # St. Martin’s\n",
    "    \"st. martin's paperbacks\": \"St. Martin's Press\",\n",
    "    \"st. martin's griffin\": \"St. Martin's Press\",\n",
    "    \"st. martin's minotaur\": \"St. Martin's Press\",\n",
    "\n",
    "    # Bantam / Doubleday / Dell / Ballantine\n",
    "    \"bantam books\": \"Bantam\",\n",
    "    \"bantam dell pub group\": \"Bantam\",\n",
    "    \"bantam dell publishing group\": \"Bantam\",\n",
    "    \"bantam doubleday dell\": \"Bantam\",\n",
    "    \"doubleday books\": \"Doubleday\",\n",
    "    \"ballantine books\": \"Ballantine\",\n",
    "\n",
    "    # Tor / Vintage / Anchor / Putnam\n",
    "    \"tor fantasy\": \"Tor Books\",\n",
    "    \"tor books (mm)\": \"Tor Books\",\n",
    "    \"tor books\": \"Tor Books\",\n",
    "    \"vintage books usa\": \"Vintage\",\n",
    "    \"vintage\": \"Vintage\",\n",
    "    \"anchor pub\": \"Anchor\",\n",
    "    \"anchor books\": \"Anchor\",\n",
    "    \"putnam pub group\": \"Putnam\",\n",
    "    \"putnam pub group (paper)\": \"Putnam\",\n",
    "    \"putnam publishing group\": \"Putnam\",\n",
    "    \"g. p. putnam's sons\": \"Putnam\",\n",
    "\n",
    "    # Scholastic / Avon / Pocket\n",
    "    \"scholastic inc\": \"Scholastic\",\n",
    "    \"scholastic\": \"Scholastic\",\n",
    "    \"avon books\": \"Avon\",\n",
    "    \"pocket books\": \"Pocket Books\",\n",
    "\n",
    "    # McGraw-Hill etc.\n",
    "    \"mc graw-hill\": \"McGraw-Hill\",\n",
    "    \"mcgraw-hill\": \"McGraw-Hill\",\n",
    "\n",
    "    # German TB variants\n",
    "    \"rowohlt tb.\": \"Rowohlt\",\n",
    "    \"rowohlt, reinbek\": \"Rowohlt\",\n",
    "    \"ullstein tb\": \"Ullstein\",\n",
    "    \"fischer (tb.), frankfurt\": \"Fischer\",\n",
    "}\n",
    "\n",
    "# labels that are clearly not useful as publishers\n",
    "DROP_SET = {\n",
    "    \"null\",\"none\",\"n/a\",\"na\",\"unknown\",\"---------\",\"—\",\"-\",\"*\", \"too far\", \"image\"\n",
    "}\n",
    "\n",
    "# chunks in parentheses/brackets to drop (format/region/city noise)\n",
    "PAREN_DROP_HINTS = re.compile(\n",
    "    r'\\b(mm|pb|paper(back)?|hardcover|mass market|uk|usa|canada|aus|ny|nyc|london|frankfurt|paris|tokyo|paper|large print)\\b',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "ACRONYM_KEEP = {\n",
    "    \"USA\",\"US\",\"UK\",\"UAE\",\"LLC\",\"INC\",\"LTD\",\"GP\",\"TB\",\"MM\"\n",
    "}\n",
    "\n",
    "def _normalize_spaces(s: str) -> str:\n",
    "    return re.sub(r'\\s{2,}', ' ', s).strip()\n",
    "\n",
    "def _strip_mojibake(s: str) -> str:\n",
    "    return None if re.search(r'[ÃÂð¼½¾þÿÞß]', s or '') else s\n",
    "\n",
    "def _strip_paren_noise(s: str) -> str:\n",
    "    # remove ()/[] chunks that look like format or location tags\n",
    "    def repl(m):\n",
    "        chunk = m.group(1)\n",
    "        return '' if PAREN_DROP_HINTS.search(chunk) else m.group(0)\n",
    "    prev = None\n",
    "    while s != prev:\n",
    "        prev = s\n",
    "        s = re.sub(r'\\(([^()]*)\\)', repl, s)\n",
    "        s = re.sub(r'\\[([^\\[\\]]*)\\]', repl, s)\n",
    "    return _normalize_spaces(s)\n",
    "\n",
    "def _std_ampersand_and_commas(s: str) -> str:\n",
    "    s = re.sub(r'\\s*&\\s*', ' & ', s)\n",
    "    s = re.sub(r'\\s*,\\s*', ', ', s)\n",
    "    s = s.replace(' ,', ',')\n",
    "    return _normalize_spaces(s)\n",
    "\n",
    "def _titlecase_preserving_acronyms(s: str) -> str:\n",
    "    out = []\n",
    "    for token in s.split():\n",
    "        raw = token.strip(\",.\")\n",
    "        if raw.upper() in ACRONYM_KEEP:\n",
    "            out.append(token.replace(raw, raw.upper()))\n",
    "        else:\n",
    "            # title-case each hyphen part\n",
    "            parts = re.split(r'(-)', token)\n",
    "            parts = [p if p == '-' else (p[:1].upper() + p[1:].lower()) for p in parts]\n",
    "            out.append(''.join(parts))\n",
    "    return ' '.join(out)\n",
    "\n",
    "def _final_tidy(s: str) -> str:\n",
    "    s = re.sub(r'\\s*\\.\\s*', '. ', s)\n",
    "    s = _normalize_spaces(s).strip(\" ,.\")\n",
    "    return s\n",
    "\n",
    "def clean_publisher(x):\n",
    "    \"\"\"Normalize publisher names, drop junk, map to canonical labels.\"\"\"\n",
    "    if not isinstance(x, str) or not x.strip():\n",
    "        return None\n",
    "    s = x.strip()\n",
    "\n",
    "    # kill mojibakey rows\n",
    "    s = _strip_mojibake(s)\n",
    "    if s is None:\n",
    "        return None\n",
    "\n",
    "    # normalize encoding / spacing\n",
    "    s = unidecode(s)\n",
    "    s = _normalize_spaces(s)\n",
    "    s = _std_ampersand_and_commas(s)\n",
    "\n",
    "    # unify 'and' to '&' when standalone word\n",
    "    s = re.sub(r'\\bAnd\\b', '&', s, flags=re.IGNORECASE)\n",
    "    s = _std_ampersand_and_commas(s)\n",
    "\n",
    "    # remove obvious format/location in parentheses/brackets\n",
    "    s = _strip_paren_noise(s)\n",
    "\n",
    "    # drop total placeholders or nearly-empty tokens\n",
    "    if s.lower() in DROP_SET or re.fullmatch(r'[-*_.]+', s):\n",
    "        return None\n",
    "\n",
    "    # light normalization of common company tokens\n",
    "    s = re.sub(r'\\bCo\\b\\.?', 'Co.', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'\\bInc\\b\\.?', 'Inc.', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'\\bLtd\\b\\.?', 'Ltd.', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'\\bPub(?:lishing)?\\s+Group\\b', 'Publishing Group', s, flags=re.IGNORECASE)\n",
    "\n",
    "    # case with acronym preservation\n",
    "    s_tc = _titlecase_preserving_acronyms(s)\n",
    "\n",
    "    # build a simple canonical key (lower, spacing unified) for mapping\n",
    "    key = s_tc.lower()\n",
    "    key = key.replace('.', '').replace(',', '').strip()\n",
    "    key = re.sub(r'\\s+', ' ', key)\n",
    "    mapped = CANON.get(key, None)\n",
    "    if mapped:\n",
    "        s_tc = mapped\n",
    "\n",
    "    s_tc = _final_tidy(s_tc)\n",
    "\n",
    "    # final guard: drop if it still looks wrong\n",
    "    if not s_tc or len(s_tc) < 2:\n",
    "        return None\n",
    "\n",
    "    return s_tc\n",
    "\n",
    "# --- apply to your df ---\n",
    "# make a new clean column (safer); drop NAs if you don't want them in viz\n",
    "books_df_cleaned['Publisher'] = books_df_cleaned['Publisher'].apply(clean_publisher)\n",
    "books_df_cleaned = books_df_cleaned.dropna(subset=['Publisher'])\n",
    "books_df_cleaned = books_df_cleaned.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53761aac-2c5e-4230-9e7f-03b1fa6e169c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Oxford University Press', 'Harper Flamingo Canada',\n",
       "       'Harper Perennial', 'Farrar Straus Giroux',\n",
       "       'W. W. Norton & Company', 'Putnam', 'Berkley Publishing Group',\n",
       "       'Audioworks', 'Random House', 'Scribner', 'Emblem Editions',\n",
       "       'Citadel Press', 'House Of Anansi Press', 'Mira Books',\n",
       "       'Health Communications', 'Brilliance Audio - Trade',\n",
       "       'Kensington Publishing Corp', 'River City Pub', 'Dell', 'Plume',\n",
       "       'Three Rivers Press', 'Ryland Peters & Small LTD', 'Cypress House',\n",
       "       'Harper Entertainment', 'Scholastic', 'Aladdin', 'Ballantine',\n",
       "       'Random House Trade Paperbacks', 'Goldmann', 'Alfred A. Knopf',\n",
       "       'Little, Brown And Company', 'HarperCollins', 'Avon', 'Bantam',\n",
       "       'Fireside', 'Harper Torch', 'Pocket', 'Tor Books',\n",
       "       'Tyndale House Publishers', 'Harvest Books',\n",
       "       'Chambers Harrap Publishers LTD', 'Speed Graphics',\n",
       "       'Sunflower Press', 'Laurel Leaf', 'Gallimard',\n",
       "       'Prentice Hall (k-12)', 'Ntc/contemporary Publishing Company',\n",
       "       \"St. Martin's Press\", '1st Books Library', 'Avon Trade',\n",
       "       'Penguin Books', 'Signet Book', 'Thomas Nelson', 'Ace Books',\n",
       "       'Landoll', 'Andrew Scott Publishers', 'Atlantic Monthly Press',\n",
       "       'Hodder & Stoughton General Division', 'Simon & Schuster',\n",
       "       'Perennial', 'McGraw-Hill', 'Minotauro', 'Distribooks',\n",
       "       'Back Bay Books', 'Doubleday', 'Warner Books'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df_cleaned['Publisher'].head(100).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8c10302-a93c-4916-abf6-3575e77ac65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common first names to help detect \"Last First\" swaps\n",
    "_COMMON_FIRSTS = {\n",
    "    \"john\",\"jane\",\"james\",\"robert\",\"michael\",\"william\",\"david\",\"richard\",\"thomas\",\"charles\",\"joseph\",\n",
    "    \"mary\",\"patricia\",\"linda\",\"barbara\",\"elizabeth\",\"jennifer\",\"maria\",\"susan\",\"margaret\",\"sarah\",\n",
    "    \"daniel\",\"paul\",\"mark\",\"donald\",\"george\",\"kenneth\",\"steven\",\"edward\",\"brian\",\"ronald\",\"anthony\",\n",
    "    \"kevin\",\"jason\",\"matthew\",\"gary\",\"timothy\",\"jose\",\"larry\",\"jeffrey\",\"frank\",\"scott\",\"eric\",\n",
    "    \"andrew\",\"stephen\",\"raymond\",\"gregory\",\"joshua\",\"jerry\",\"dennis\",\"walter\",\"patrick\",\"peter\",\n",
    "    \"harry\",\"helen\",\"emma\",\"sophie\",\"nicole\",\"anna\",\"karen\",\"carol\",\"ruth\",\"amy\",\"diane\",\"julie\",\n",
    "    \"lois\",\"laura\",\"ian\",\"louis\",\"luke\",\"nick\",\"joan\",\"jack\",\"jean\",\"anne\",\"anna\",\"hugh\",\"keith\",\n",
    "    \"olivia\",\"tanya\",\"sylvia\",\"rachel\",\"megan\",\"katherine\",\"kathleen\",\"edward\",\"arthur\",\"alan\",\n",
    "    \"victoria\",\"julia\",\"susan\",\"alison\",\"allison\",\"judith\",\"joanna\",\"alex\",\"alexander\",\"niccolo\",\n",
    "    \"vladimir\",\"paolo\",\"peter\",\"emily\",\"henry\",\"samuel\",\"philip\",\"phillip\",\"andrea\"\n",
    "}\n",
    "\n",
    "# broadened corporate / series detector\n",
    "_CORP_PATTERNS = re.compile(\n",
    "    r'\\b('\n",
    "    r'press|publisher|publishers|publications|company|co\\.?|inc\\.?|llc|ltd\\.?|'   # business\n",
    "    r'foundation|society|association|committee|institute|dept|department|'        # orgs\n",
    "    r'library|series|digest|magazine|readers?|family|circle|'                      # series/mags\n",
    "    r'guides?|workbook|handbook|companion|annual|yearbook'                         # non-person-ish\n",
    "    r')\\b', re.IGNORECASE\n",
    ")\n",
    "\n",
    "# things like \\\" or \\'\n",
    "_ESCAPED_QUOTES = re.compile(r'\\\\([\\'\\\"])')\n",
    "# remove bracketed junk: \"Name (Editor)\" , \"Smith [Ed.]\" , \"Foo {Series}\"\n",
    "_PARENS_BLOCK = re.compile(r'[\\(\\[\\{].*?[\\)\\]\\}]')\n",
    "# \"H+J Rousseau\" -> \"H. J. Rousseau\"\n",
    "_PLUS_INITIALS = re.compile(r'\\b([A-Za-z])\\s*\\+\\s*([A-Za-z])\\b')\n",
    "# split multi-author strings (keep first by default)\n",
    "_MULTI_AUTHOR_SPLIT = re.compile(r'\\s*(?:/|;|&| and |, and )\\s+', re.IGNORECASE)\n",
    "\n",
    "_PLACEHOLDERS = {\n",
    "    \"read\",\"n/a\",\"na\",\"not available\",\"not applicable\",\"varios\",\"various\",\"tk\"\n",
    "}\n",
    "_EDITORS_OF = re.compile(r'^\\s*(the\\s+)?editors?\\s+of\\b', re.IGNORECASE)\n",
    "\n",
    "# degrees / suffixes to strip\n",
    "_DEGREES = re.compile(\n",
    "    r'(?:(?<!\\S)(dr|mrs|mr|ms|prof)\\.?\\s+)|'          # titles at start\n",
    "    r'(\\s*,?\\s*(ph\\.?\\s*d\\.?|m\\.?\\s*d\\.?|md|mba|jd|esq\\.?|dphil|dd|dds|rn|iii|ii|iv|jr\\.?|sr\\.?)\\s*$)',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "_PARTICLES_LOWER = {\"de\",\"del\",\"de la\",\"de los\",\"du\",\"da\",\"dos\",\"van\",\"von\",\"der\",\"den\",\"la\",\"le\",\"di\"}\n",
    "\n",
    "_INITIAL = re.compile(r'^[A-Z]$')\n",
    "_INITIAL_WITH_DOT = re.compile(r'^[A-Z]\\.$')\n",
    "\n",
    "def _tidy_spaces(s: str) -> str:\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "def _fix_apostrophes(s: str) -> str:\n",
    "    # normalize weird \"O.'dell\" -> \"O'Dell\"\n",
    "    s = re.sub(r\"\\bO\\.\\s*'([a-z])\", lambda m: \"O'\" + m.group(1).upper(), s)\n",
    "    # normalize smart/odd quotes\n",
    "    s = s.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "    # Title-case the letter after O'\n",
    "    s = re.sub(r\"\\bO'([a-z])\", lambda m: \"O'\" + m.group(1).upper(), s)\n",
    "    # Fix weird caps like O.'Dell already handled; also d'Este, D'Estate\n",
    "    s = re.sub(r\"\\b([A-Za-z])'([a-z])\", lambda m: m.group(1) + \"'\" + m.group(2).upper(), s)\n",
    "    return s\n",
    "\n",
    "def _fix_mc_names(token: str) -> str:\n",
    "    # Join \"Mc Murtry\" -> \"McMurtry\"\n",
    "    token = re.sub(r'\\bMc\\s+([A-Za-z])', lambda m: 'Mc' + m.group(1).upper(), token)\n",
    "    # \"Mccarthy\" -> \"McCarthy\"\n",
    "    token = re.sub(r'\\bMcc([a-z])', lambda m: 'McC' + m.group(1), token)\n",
    "    return token\n",
    "\n",
    "def _ensure_initials(tokens):\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        core = t.strip(\".\")\n",
    "        if len(core) == 1 and core.isalpha():\n",
    "            out.append(core.upper() + \".\")\n",
    "        else:\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "def _titlecase_name_piece(piece: str) -> str:\n",
    "    # keep hyphenated and apostrophe chunks titlecased appropriately\n",
    "    def tc(word: str) -> str:\n",
    "        if _INITIAL.match(word) or _INITIAL_WITH_DOT.match(word):\n",
    "            return word.upper() if len(word) == 1 else word.upper()[0] + \".\"\n",
    "        return word[:1].upper() + word[1:].lower()\n",
    "    # handle apostrophes (O'Dell) and hyphens (Jean-Baptiste)\n",
    "    piece = \"-\".join(tc(p) for p in piece.split(\"-\"))\n",
    "    piece = \"'\".join(tc(p) for p in piece.split(\"'\"))\n",
    "    # Mc/Mac tweaks after basic case\n",
    "    piece = _fix_mc_names(piece)\n",
    "    return piece\n",
    "\n",
    "def _smart_titlecase_full(name: str) -> str:\n",
    "    parts = name.split()\n",
    "    parts = _ensure_initials(parts)\n",
    "    # rebuild with particles lowered except first/last when standalone multiword particles\n",
    "    final = []\n",
    "    i = 0\n",
    "    while i < len(parts):\n",
    "        token = parts[i]\n",
    "        raw = token\n",
    "        low = raw.lower().strip(\".\")\n",
    "        # multiword particles like \"de la\"\n",
    "        if i+1 < len(parts) and f\"{low} {parts[i+1].lower().strip('.')}\" in _PARTICLES_LOWER:\n",
    "            combo = f\"{low} {parts[i+1].lower().strip('.')}\"\n",
    "            final.append(combo)\n",
    "            i += 2\n",
    "            continue\n",
    "        if low in _PARTICLES_LOWER:\n",
    "            final.append(low)\n",
    "        else:\n",
    "            final.append(_titlecase_name_piece(raw))\n",
    "        i += 1\n",
    "    # ensure first and last token not left in lowercase particle by accident\n",
    "    if final:\n",
    "        final[0] = _titlecase_name_piece(final[0]) if final[0].lower() in _PARTICLES_LOWER else final[0]\n",
    "    if len(final) > 1 and final[-1].lower() in _PARTICLES_LOWER:\n",
    "        final[-1] = _titlecase_name_piece(final[-1])\n",
    "    return \" \".join(final)\n",
    "\n",
    "def _looks_corporate(s: str) -> bool:\n",
    "    if _EDITORS_OF.search(s):\n",
    "        return True\n",
    "    if _CORP_PATTERNS.search(s):\n",
    "        return True\n",
    "    if s.strip().lower() in _PLACEHOLDERS:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _strip_degrees_titles(s: str) -> str:\n",
    "    prev = None\n",
    "    while s != prev:\n",
    "        prev = s\n",
    "        s = _DEGREES.sub(lambda m: '' if m.group(0) else '', s).strip(\" ,\")\n",
    "    return s\n",
    "\n",
    "def _flip_last_first(s: str) -> str:\n",
    "    # \"Last, First M.\" -> \"First M. Last\"\n",
    "    if \",\" in s:\n",
    "        parts = [p.strip() for p in s.split(\",\")]\n",
    "        if len(parts) >= 2:\n",
    "            left = parts[0]\n",
    "            right = \" \".join(parts[1:]).strip()\n",
    "            if left and right:\n",
    "                return f\"{right} {left}\"\n",
    "    # \"Kipling Rudyard\" -> \"Rudyard Kipling\" if second looks like first name\n",
    "    toks = s.split()\n",
    "    if len(toks) == 2:\n",
    "        a, b = toks[0], toks[1]\n",
    "        if b.lower().strip(\".\") in _COMMON_FIRSTS and a[0].isupper() and b[0].isupper():\n",
    "            return f\"{b} {a}\"\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_author(author: str, *, keep_first_author: bool = True) -> str | None:\n",
    "    \"\"\"Return a cleaned, person-like author name or None if it looks non-person/placeholder.\"\"\"\n",
    "    if not isinstance(author, str) or not author.strip():\n",
    "        return None\n",
    "\n",
    "    s = author.strip()\n",
    "\n",
    "    # 0) Drop bracketed chunks early (e.g., \"(Editor)\", \"[Ed.]\", \"{Series}\")\n",
    "    s = _PARENS_BLOCK.sub(\" \", s)\n",
    "\n",
    "    # 1) Unescape things like \\\" and \\'\n",
    "    s = _ESCAPED_QUOTES.sub(r'\\1', s)\n",
    "\n",
    "    # 2) Normalize HTML & stray entities and Unicode\n",
    "    s = s.replace(\"&amp;\", \"&\")\n",
    "    s = unidecode(s)\n",
    "\n",
    "    # 3) If multi-author string, optionally keep the first person only\n",
    "    if keep_first_author and _MULTI_AUTHOR_SPLIT.search(s):\n",
    "        s = _MULTI_AUTHOR_SPLIT.split(s, maxsplit=1)[0]\n",
    "\n",
    "    # 4) Quick tidy of spacing\n",
    "    s = _tidy_spaces(s)\n",
    "\n",
    "    # 5) Obvious non-persons / placeholders\n",
    "    if _looks_corporate(s) or s.lower() in _PLACEHOLDERS:\n",
    "        return None\n",
    "\n",
    "    # 6) Strip degrees/titles at ends or start\n",
    "    s = _strip_degrees_titles(s)\n",
    "\n",
    "    # 7) Flip \"Last, First\" or common \"Last First\" cases\n",
    "    s = _flip_last_first(s)\n",
    "\n",
    "    # 8) Normalize weird punctuation/casing\n",
    "    #    - \"H+J\" -> \"H. J.\"\n",
    "    s = _PLUS_INITIALS.sub(lambda m: f\"{m.group(1).upper()}. {m.group(2).upper()}.\", s)\n",
    "    #    - tidy spaced periods, normalize apostrophes & Mc/Mac\n",
    "    s = _fix_apostrophes(s)\n",
    "    s = re.sub(r'\\s*\\.\\s*\\.\\s*', '..', s)\n",
    "    s = _fix_mc_names(s)\n",
    "    s = _tidy_spaces(s)\n",
    "\n",
    "    # 9) Standardize initials spacing: \"J R R Tolkien\" -> \"J. R. R. Tolkien\"\n",
    "    tokens = _ensure_initials(s.split())\n",
    "    s = \" \".join(tokens)\n",
    "\n",
    "    # 10) Final smart titlecase with particles handled\n",
    "    s = _smart_titlecase_full(s).strip(\" ,.;-\")\n",
    "    s = _tidy_spaces(s)\n",
    "\n",
    "    # 11) Prune obvious single-word junk\n",
    "    bad_singletons = {\"read\", \"christie\", \"watterson\", \"barrie\", \"aawdri\", \"awdry\"}\n",
    "    if \" \" not in s and s.lower() in bad_singletons:\n",
    "        return None\n",
    "\n",
    "    # 12) Minimum signal (avoid “Abc”, “A”)\n",
    "    if len(s) < 3:\n",
    "        return None\n",
    "\n",
    "    return s or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c1e8852-5d5f-4b6d-a8f1-98bce3d70128",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_cleaned['Book-Author'] = books_df_cleaned['Book-Author'].apply(clean_author)\n",
    "books_df_cleaned = books_df_cleaned.dropna(subset=['Book-Author'])\n",
    "books_df_cleaned = books_df_cleaned.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d09b789-06b6-4d6c-9b9e-12e476ea1054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mark P. O. Morford', 'Richard Bruce Wright', \"Carlo D'Este\",\n",
       "       'Gina Bari Kolata', 'E. J. W. Barber', 'Amy Tan', 'Robert Cowley',\n",
       "       'Scott Turow', 'David Cordingly', 'Ann Beattie',\n",
       "       'David Adams Richards', 'Adam Lebor', 'Sheila Heti',\n",
       "       'R. J. Kaiser', 'Jack Canfield', 'Loren D. Estleman',\n",
       "       'Robert Hendrickson', 'Julia Oliver', 'John Grisham',\n",
       "       'Toni Morrison', 'The Onion', 'Celia Brooks Brown',\n",
       "       'J. R. Parrish', 'Mary-kate', 'Robynn Clairday', 'Kathleen Duey',\n",
       "       'Michael Crichton', 'C.s. Lewis', 'Arthur Phillips',\n",
       "       'Stephan Jaramillo', 'Eleanor Cooney', 'Charlotte Link',\n",
       "       'Richard North Patterson', 'Mark Salzman', 'Harper Lee',\n",
       "       'Laura Hillenbrand', 'Barbara Kingsolver', 'Jo Dereske',\n",
       "       'Jane Austen', 'Dolores Krieger', 'Anne Rivers Siddons',\n",
       "       'Dean R. Koontz', 'Mary Higgins Clark', 'Dean Koontz',\n",
       "       'Patricia Cornwell', 'J.d. Robb', 'Maeve Binchy', 'Laura J. Mixon',\n",
       "       'Tim Lahaye', 'M.d. Bernie S. Siegel', 'Robert Penn Warren',\n",
       "       'Hans Johannes Hoefer', 'Mark Helprin', 'O. Carol Simonton',\n",
       "       'Chuck Hill', 'David Iglehart', 'Larry Mcmurtry',\n",
       "       'Suzanne Fisher Staples', 'Michel Tournier', 'Carl Sagan',\n",
       "       'Aleksandr Zinoviev', 'Anne Tyler', 'Joseph Conrad',\n",
       "       'Deepak Chopra', 'Thomas Hardy', 'Charles Noland',\n",
       "       'Valerie Frankel', 'Benjamin Hoff', 'Niccolo Machiavelli',\n",
       "       'H. Jackson Brown', 'Robert A. Heinlein', 'Philip Pullman',\n",
       "       'Anna Sewell', 'Michael Ondaatje', 'Sandra Levy Ceren',\n",
       "       \"P.j. O'Rourke\", 'Mike Gayle', 'Stel Pavlou', 'Sarah Payne Stuart',\n",
       "       'Dan Quayle', 'Donald F. Kettl', 'David Frum', 'Louis Lamour',\n",
       "       'J.d. Salinger', 'J. R. R. Tolkien', 'John Berendt',\n",
       "       'Jennifer Crusie', 'Jane Heller', 'Michael Rips', 'Simon Mawer',\n",
       "       'William Abrahams', 'Robert T. Kiyosaki'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df_cleaned['Book-Author'].head(100).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6500cbf2-445e-4e60-a1ad-4a545de01e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACRONYMS = {\n",
    "    \"USA\",\"US\",\"UK\",\"UAE\",\"UN\",\"EU\",\"NATO\",\"GCSE\",\"SAT\",\"ACT\",\"GMAT\",\"GRE\",\n",
    "    \"DC\",\"NYC\",\"L.A.\",\"TV\",\"AI\",\"DNA\",\"RNA\",\"U.S.\",\"U.K.\",\"II\",\"III\",\"IV\",\"VI\",\"VII\",\"VIII\",\"IX\",\"X\"\n",
    "}\n",
    "ROMAN_RE = re.compile(r'^(?=[MDCLXVI])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$', re.IGNORECASE)\n",
    "\n",
    "# broader “series/edition” hints (added Shadowrun, Dragonlance, Boxed Set, etc.)\n",
    "SERIES_HINTS = [\n",
    "    'series','omnibus','library','edition','ed.','large print','paperback','hardcover',\n",
    "    'voyager ser','lost classics','collection','collector','vol.','volume','vol','box set','boxed set',\n",
    "    \"world's classics\",'classic','guild','annotated','unabridged','illustrated','mass market',\n",
    "    'shadowrun','dragonlance','forgotten realms','star wars','lotr','lord of the rings'\n",
    "]\n",
    "\n",
    "SMALL_WORDS = {\n",
    "    'a','an','and','as','at','but','by','for','in','nor','of','on','or','per','the','to','via',\n",
    "    # Spanish small words\n",
    "    'de','del','la','el','los','las','y','o','en','un','una','unos','unas','por','con','sin','al'\n",
    "}\n",
    "\n",
    "# obvious junk we should drop if the whole title equals one of these (case-insensitive)\n",
    "BAD_TITLE_SET = {'null','n/a','na','untitled','unknown','abc'}\n",
    "\n",
    "# fast checks for “almost nothing” titles\n",
    "_ONLY_PUNCT_OR_DIGITS = re.compile(r'^[\\W_0-9]+$')\n",
    "\n",
    "_ESCAPED_QUOTES = re.compile(r'\\\\([\\'\\\"])')               # \\\"  -> \"\n",
    "_REPEAT_PUNCT = re.compile(r'([!?.,:;]){2,}')             # \"!!!\" -> \"!\"\n",
    "_EXTRA_SPACES = re.compile(r'\\s{2,}')                     # collapse multi spaces\n",
    "\n",
    "def _smart_token_case(token: str) -> str:\n",
    "    core = token.strip(\".,:;!?\\\"'()[]{}\")\n",
    "    if not core:\n",
    "        return token\n",
    "    if core.upper() in ACRONYMS or ROMAN_RE.match(core):\n",
    "        return token.replace(core, core.upper())\n",
    "    # keep possessives nice: O'Brien's -> O'Brien's, CHILDREN'S -> Children's\n",
    "    token = re.sub(r\"\\'S\\b\", \"'s\", token)\n",
    "    return token\n",
    "\n",
    "def _strip_series_parens(text: str) -> str:\n",
    "    \"\"\"Remove parenthesized/bracketed chunks that look like series/edition/format metadata.\"\"\"\n",
    "    def looks_like_series(chunk: str) -> bool:\n",
    "        c = chunk.lower().strip()\n",
    "        if any(h in c for h in SERIES_HINTS):\n",
    "            return True\n",
    "        # heuristics: lots of non-words or mostly meta words\n",
    "        meta = {'novel','paperback','hardcover','edition','volume','vol','book','books',\n",
    "                'spanish','french','german','italian','translated','translation',\n",
    "                'complete','illustrated','annotated','revised','abridged'}\n",
    "        words = re.findall(r\"[a-zA-Z]+\", c)\n",
    "        if not words:\n",
    "            return True\n",
    "        score = sum(w in meta for w in words)/len(words)\n",
    "        return score >= 0.5\n",
    "\n",
    "    s = text\n",
    "    prev = None\n",
    "    # iterate until stable (handles nested/adjacent brackets)\n",
    "    while s != prev:\n",
    "        prev = s\n",
    "        s = re.sub(r'\\(([^()]*)\\)', lambda m: '' if looks_like_series(m.group(1)) else m.group(0), s)\n",
    "        s = re.sub(r'\\[([^\\[\\]]*)\\]', lambda m: '' if looks_like_series(m.group(1)) else m.group(0), s)\n",
    "        s = re.sub(r'\\{([^{}]*)\\}',  lambda m: '' if looks_like_series(m.group(1)) else m.group(0), s)\n",
    "        s = _EXTRA_SPACES.sub(' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def _fix_contractions(s: str) -> str:\n",
    "    # Children'S -> Children's, It'S -> It's\n",
    "    return re.sub(r\"\\b([A-Za-z]+)'([A-Z])([a-z]*)\\b\",\n",
    "                  lambda m: f\"{m.group(1)}'{m.group(2).lower()}{m.group(3)}\", s)\n",
    "\n",
    "def _fix_spanish_articles(tokens):\n",
    "    out = []\n",
    "    for i, t in enumerate(tokens):\n",
    "        core = t.strip(\".,:;!?\\\"'()[]{}\")\n",
    "        if core.upper() == \"LA\" and i+1 < len(tokens) and tokens[i+1][:1].islower():\n",
    "            t = t.replace(core, \"La\")\n",
    "        out.append(t)\n",
    "    return out\n",
    "\n",
    "def _apply_small_words_title_case(tokens):\n",
    "    if not tokens: \n",
    "        return tokens\n",
    "    new = []\n",
    "    for i, t in enumerate(tokens):\n",
    "        core = t.strip(\".,:;!?\\\"'()[]{}\")\n",
    "        low = core.lower()\n",
    "        if 0 < i < len(tokens)-1 and low in SMALL_WORDS and core.upper() not in ACRONYMS and not ROMAN_RE.match(core):\n",
    "            t = t.replace(core, low)\n",
    "        new.append(t)\n",
    "    return new\n",
    "\n",
    "def _capitalize_after_punct(s: str) -> str:\n",
    "    # after colon or em dash, capitalize next letter\n",
    "    s = re.sub(r'(:\\s+)([a-z])', lambda m: m.group(1)+m.group(2).upper(), s)\n",
    "    s = re.sub(r'(—\\s+)([a-z])', lambda m: m.group(1)+m.group(2).upper(), s)\n",
    "    return s\n",
    "\n",
    "def _strip_trailing_series_words(s: str) -> str:\n",
    "    return re.sub(r'\\b(?:collection|collections|omnibus|box set|boxed set|annotated edition)\\b\\.?$', '', s, flags=re.IGNORECASE).strip()\n",
    "\n",
    "def _spanish_specific_fixes(s: str) -> str:\n",
    "    s = re.sub(r'\\bEl Senor De Los Anillos\\b', 'El Senor de los Anillos', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'\\bLa Comunidad Del Anillo\\b', 'La Comunidad del Anillo', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'\\bLas Dos Torres\\b', 'Las Dos Torres', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'\\bEl Retorno Del Rey\\b', 'El Retorno del Rey', s, flags=re.IGNORECASE)\n",
    "    return s\n",
    "\n",
    "# --- main cleaner ---\n",
    "\n",
    "def clean_title(title: str, drop_series: bool = True) -> str | None:\n",
    "    \"\"\"Return a normalized, human title or None if it looks like garbage.\"\"\"\n",
    "    if not isinstance(title, str) or not title.strip():\n",
    "        return None\n",
    "\n",
    "    s = title.strip()\n",
    "\n",
    "    # obvious mojibake — punt it\n",
    "    if re.search(r'[ÃÂð¼½¾þÿÞß]', s):\n",
    "        return None\n",
    "\n",
    "    # normalize escapes/encoding\n",
    "    s = _ESCAPED_QUOTES.sub(r'\\1', s)     # \\\" -> \"\n",
    "    s = unidecode(s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "    # ditch total placeholders\n",
    "    if s.lower() in BAD_TITLE_SET:\n",
    "        return None\n",
    "\n",
    "    # punctuation hygiene\n",
    "    s = _REPEAT_PUNCT.sub(lambda m: m.group(1), s)\n",
    "    s = re.sub(r'\\s*:\\s*', ': ', s)\n",
    "    s = re.sub(r'\\s*/\\s*', ' / ', s)\n",
    "    s = re.sub(r'\\s+([,.;:!?])', r'\\1', s)\n",
    "    s = re.sub(r'([(\\[]) +', r'\\1', s)\n",
    "    s = re.sub(r' +([)\\]])', r'\\1', s)\n",
    "    # hyphen spacing: letters around hyphen stick; digit-digit becomes en-dash\n",
    "    s = re.sub(r'(\\d)\\s*-\\s*(\\d)', r'\\1–\\2', s)\n",
    "    s = re.sub(r'([A-Za-z])\\s*-\\s*([A-Za-z])', r'\\1-\\2', s)\n",
    "\n",
    "    # contractions\n",
    "    s = _fix_contractions(s)\n",
    "\n",
    "    # strip parenthesized/bracketed series/format\n",
    "    if drop_series:\n",
    "        s = _strip_series_parens(s)\n",
    "\n",
    "    # token-level casing\n",
    "    tokens = s.split(' ')\n",
    "    tokens = [_smart_token_case(t) for t in tokens]\n",
    "    tokens = _fix_spanish_articles(tokens)\n",
    "    tokens = _apply_small_words_title_case(tokens)\n",
    "    s = ' '.join(tokens)\n",
    "\n",
    "    # polish\n",
    "    s = _capitalize_after_punct(s)\n",
    "    s = _strip_trailing_series_words(s)\n",
    "    s = _spanish_specific_fixes(s)\n",
    "    s = _EXTRA_SPACES.sub(' ', s).strip(' -.,;:').strip()\n",
    "\n",
    "    # final safety: drop if still garbagey\n",
    "    if not s or _ONLY_PUNCT_OR_DIGITS.match(s):\n",
    "        return None\n",
    "    if len(re.sub(r'[^A-Za-z]', '', s)) < 2:      # not enough letters to be a book\n",
    "        return None\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2612efa-1592-441c-8663-8f76ae7d7cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_cleaned['Book-Title'] = books_df_cleaned['Book-Title'].apply(clean_title)\n",
    "books_df_cleaned = books_df_cleaned.dropna(subset=['Book-Title'])\n",
    "books_df_cleaned = books_df_cleaned.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a54e3c3-bc30-448d-b2aa-33eb20500e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Classical Mythology', 'Clara Callan', 'Decision in Normandy',\n",
       "       'Flu: The Story of the Great Influenza Pandemic of 1918 and the Search for the Virus That Caused It',\n",
       "       'The Mummies of Urumchi', \"The Kitchen God's Wife\",\n",
       "       \"What If: The World's Foremost Military Historians Imagine What Might Have Been\",\n",
       "       'Pleading Guilty',\n",
       "       'Under the Black Flag: The Romance and the Reality of Life Among the Pirates',\n",
       "       \"Where You'll Find Me: And Other Stories\",\n",
       "       'Nights Below Station Street',\n",
       "       \"Hitler's Secret Bankers: The Myth of Swiss Neutrality During the Holocaust\",\n",
       "       'The Middle Stories', 'Jane Doe',\n",
       "       \"A Second Chicken Soup for the Woman's Soul\", 'The Witchfinder',\n",
       "       'More Cunning Than Man: A Social History of Rats and Man',\n",
       "       'Goodbye to the Buttermilk Sky', 'The Testament',\n",
       "       'Beloved (Plume Contemporary Fiction)',\n",
       "       \"Our Dumb Century: The Onion Presents 100 Years of Headlines From America's Finest News Source\",\n",
       "       'New Vegetarian: Bold and Beautiful Recipes for Every Occasion',\n",
       "       \"If I'd Known Then What I Know Now: Why Not Learn From the Mistakes of Others?: You Can't Afford to Make Them All Yourself\",\n",
       "       'Mary-Kate & Ashley Switching Goals (Mary-Kate and Ashley Starring In)',\n",
       "       \"Tell Me This Isn't Happening\", 'Flood: Mississippi 1927',\n",
       "       'Airframe', 'Timeline', 'Out of the Silent Planet',\n",
       "       'Prague: A Novel', 'Chocolate Jesus',\n",
       "       'Der Fluch Der Kaiserin. Ein Richter-Di-Roman', 'Sturmzeit. Roman',\n",
       "       'Tage Der Unschuld', 'Lying Awake', 'To Kill a Mockingbird',\n",
       "       'Seabiscuit: An American Legend', 'Pigs in Heaven',\n",
       "       \"Miss Zukas and the Raven's Dance\", 'Pride and Prejudice',\n",
       "       'The Therapeutic Touch: How to Use Your Hands to Help or to Heal',\n",
       "       'Downtown', 'Icebound', \"I'll Be Seeing You\",\n",
       "       'From the Corner of His Eye', 'Isle of Dogs', 'Purity in Death',\n",
       "       'This Year It Will Be Different: And Other Stories', 'Proxies',\n",
       "       \"Left Behind: A Novel of the Earth's Last Days (Left Behind #1)\",\n",
       "       'The Street Lawyer', 'Love, Medicine and Miracles',\n",
       "       \"All the King's Men\", 'Pacific Northwest',\n",
       "       'A Soldier of the Great War', 'Getting Well Again',\n",
       "       'Northwest Wines and Wineries',\n",
       "       'An Atmosphere of Eternity: Stories of India', 'Lonesome Dove',\n",
       "       'Shabanu: Daughter of the Wind (Border Trilogy)',\n",
       "       'Haveli (Laurel Leaf Books)', 'Lieux Dits',\n",
       "       'The Dragons of Eden: Speculations on the Evolution of Human Intelligence',\n",
       "       'The Yawning Heights', 'Breathing Lessons', 'The Joy Luck Club',\n",
       "       'Heart of Darkness', 'The Angel Is Near',\n",
       "       \"Tess of the D'urbervilles\",\n",
       "       'The Adventures of Drew and Ellie: The Magical Dress',\n",
       "       'The Accidental Virgin', 'The Tao of Pooh', 'Seabiscuit',\n",
       "       'The Prince',\n",
       "       \"Life's Little Instruction Book (Life's Little Instruction Books )\",\n",
       "       'Starship Troopers',\n",
       "       'The Ruby in the Smoke (Sally Lockhart Trilogy, Book 1)',\n",
       "       'Black Beauty', \"Anil's Ghost\", 'Prescription for Terror',\n",
       "       'Modern Manners: An Etiquette Book for Rude People',\n",
       "       'Turning Thirty', 'Decipher',\n",
       "       'My First Cousin Once Removed: Money, Madness, and the Family of Robert Lowell',\n",
       "       'Standing Firm: A Vice-Presidential Memoir',\n",
       "       'Team Bush: Leadership Lessons From the Bush White House',\n",
       "       'The Right Man: The Surprise Presidency of George W. Bush',\n",
       "       'Daybreakers Louis Lamour', 'The Catcher in the Rye',\n",
       "       'El Senor de los Anillos: La Comunidad del Anillo',\n",
       "       'El Senor de los Anillos: Las Dos Torres',\n",
       "       'El Senor de los Anillos: El Retorno del Rey',\n",
       "       'Midnight in the Garden of Good and Evil: A Savannah Story',\n",
       "       \"Pretend You Don't See Her\", 'Fast Women', 'Female Intelligence',\n",
       "       \"Pasquale's Nose: Idle Days in an Italian Town\",\n",
       "       'The Gospel of Judas: A Novel',\n",
       "       \"Prize Stories, 1987: The O'henry Awards\",\n",
       "       'Rich Dad, Poor Dad: What the Rich Teach Their Kids About Money--That the Poor and Middle Class Do Not!'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df_cleaned['Book-Title'].head(100).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5045ab4d-a906-4f18-81ec-68f88c1dbcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>Stockton</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>Albacete</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>Langhorne</td>\n",
       "      <td>pennsylvania</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>quebec</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "      <td>Viterbo</td>\n",
       "      <td>lazio</td>\n",
       "      <td>italy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID  Age       City          State  Country\n",
       "0        2   18   Stockton     california      usa\n",
       "1       10   26   Albacete      wisconsin    spain\n",
       "2       20   19  Langhorne   pennsylvania      usa\n",
       "3       36   24   Montreal         quebec   canada\n",
       "4       38   34    Viterbo          lazio    italy"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with short or clearly invalid city names\n",
    "invalid_city_patterns = ['-', '.', '...', 'X', '_____', '~*~', '---------']\n",
    "\n",
    "users_df = users_df[~users_df['City'].isin(invalid_city_patterns)]\n",
    "\n",
    "users_df = users_df[users_df['City'].str.len() > 2]\n",
    "\n",
    "users_df['City'] = users_df['City'].str.replace(r'\\s*\\(.*?\\)', '', regex=True).str.strip().str.title()\n",
    "\n",
    "users_df['City'] = users_df['City'].str.split('/').str[0].str.strip()\n",
    "\n",
    "# Define function to detect invalid city entries\n",
    "def is_invalid_city(city):\n",
    "    if pd.isna(city):\n",
    "        return True\n",
    "    city = str(city).strip()\n",
    "    return (\n",
    "        city.lower() in garbage_set or\n",
    "        bool(re.match(r\"^\\d{4,6}$\", city)) or                 # exact postal code\n",
    "        bool(re.match(r\"^\\d+\\s\", city)) or                    # starts with a number\n",
    "        bool(re.search(r\"\\d{4,6}\", city)) or                  # contains long number\n",
    "        bool(re.match(r\"^\\d{5}\\s+\\w+\", city)) or              # \"23564 Lubeck\"\n",
    "        len(city) < 3                                         # too short\n",
    "    )\n",
    "\n",
    "# Known garbage city values (lowercase for easier comparison)\n",
    "garbage_values = {\n",
    "    'unknown', 'unk', 'n/a', 'na', 'none', 'null',\n",
    "    'abc', 'a town', '', '.', '..', '00000', '0'\n",
    "}\n",
    "\n",
    "# Convert to lowercase for consistent matching\n",
    "garbage_set = set(val.lower() for val in garbage_values)\n",
    "\n",
    "# Apply the cleaning: set invalid cities to <NA>\n",
    "users_df['City'] = users_df['City'].apply(\n",
    "    lambda x: pd.NA if is_invalid_city(x) else str(x).strip().title()\n",
    ")\n",
    "\n",
    "# Optional final cleanup (just in case)\n",
    "users_df['City'] = users_df['City'].str.replace(r\"[^a-zA-ZÀ-ÿ\\s\\'\\-]\", \"\", regex=True).str.strip()\n",
    "\n",
    "users_df[['City', 'State', 'Country']] = users_df[['City', 'State', 'Country']].replace(\n",
    "    to_replace=r'^\\s*$|^[Uu]nknown$|^,\\s*$',\n",
    "    value=pd.NA,\n",
    "    regex=True\n",
    ")\n",
    "\n",
    "users_df = users_df.dropna(subset=['City', 'State', 'Country'])\n",
    "users_df = users_df.reset_index(drop=True)\n",
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d52c008-6128-40c4-81b7-b87324c376da",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df['city_corrupt'] = users_df['City'].apply(has_non_ascii)\n",
    "users_df['state_corrupt'] = users_df['State'].apply(has_non_ascii)\n",
    "users_df['country_corrupt'] = users_df['Country'].apply(has_non_ascii)\n",
    "\n",
    "users_df['short_city'] = users_df['City'].str.strip().str.len() <= 2\n",
    "users_df['short_state'] = users_df['State'].str.strip().str.len() <= 2\n",
    "users_df['short_country'] = users_df['Country'].str.strip().str.len() <= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c823211-5f52-4c98-be27-86239d1188ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define flag columns and count the number of them to drop fields if there are more than or equal to 2 flags\n",
    "flags = ['city_corrupt', 'state_corrupt', 'country_corrupt', 'short_city', 'short_state', 'short_country']\n",
    "users_df['bad_count'] = users_df[flags].sum(axis=1)\n",
    "users_df_cleaned = users_df[users_df['bad_count'] < 2].drop(columns=flags + ['bad_count']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e106700c-054c-4634-a3e3-6ab645d781d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 59898\n",
      "After: 59555\n",
      "Dropped: 343\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", len(users_df))\n",
    "print(\"After:\", len(users_df_cleaned))\n",
    "print(\"Dropped:\", len(users_df) - len(users_df_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7315a963-b1e6-4e05-ac39-18c6630f0380",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_cleaned['City'] = users_df_cleaned['City'].str.title().str.strip()\n",
    "users_df_cleaned['State'] = users_df_cleaned['State'].str.title().str.strip()\n",
    "users_df_cleaned['Country'] = users_df_cleaned['Country'].str.title().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df0d3ab4-df98-4bd3-9d5d-b885c59988ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>Stockton</td>\n",
       "      <td>California</td>\n",
       "      <td>Usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>Albacete</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>Langhorne</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>Quebec</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "      <td>Viterbo</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID  Age       City         State Country\n",
       "0        2   18   Stockton    California     Usa\n",
       "1       10   26   Albacete     Wisconsin   Spain\n",
       "2       20   19  Langhorne  Pennsylvania     Usa\n",
       "3       36   24   Montreal        Quebec  Canada\n",
       "4       38   34    Viterbo         Lazio   Italy"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eaa0db8a-7e4c-481e-8d11-60332117b8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>Stockton</td>\n",
       "      <td>California</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>Albacete</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>Langhorne</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>Quebec</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "      <td>Viterbo</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID  Age       City         State        Country\n",
       "0        2   18   Stockton    California  United States\n",
       "1       10   26   Albacete     Wisconsin          Spain\n",
       "2       20   19  Langhorne  Pennsylvania  United States\n",
       "3       36   24   Montreal        Quebec         Canada\n",
       "4       38   34    Viterbo         Lazio          Italy"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only the last part after the last comma (assuming it's the country)\n",
    "users_df_cleaned['Country'] = users_df_cleaned['Country'].str.split(',').str[-1].str.strip()\n",
    "\n",
    "# Remove extra quotes and non-letter characters\n",
    "users_df_cleaned['Country'] = users_df_cleaned['Country'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "\n",
    "country_fixes = {\n",
    "    'Everywhere And Anywhere': pd.NA,\n",
    "    'Here And There': pd.NA,\n",
    "    'Hungary And Usa': pd.NA,\n",
    "    'The': pd.NA,\n",
    "    'United State': 'United States',\n",
    "    'Us': 'United States',\n",
    "    'USA': 'United States',\n",
    "    'Usa': 'United States',\n",
    "    'USA\"': 'United States',\n",
    "    'Usa & Canada': 'United States',\n",
    "    'Phillipines': 'Philippines',\n",
    "    'Philippines\"': 'Philippines',\n",
    "    'Uae': 'United Arab Emirates',\n",
    "    'U.A.E': 'United Arab Emirates',\n",
    "    'UAE': 'United Arab Emirates',\n",
    "    'UK': 'United Kingdom',\n",
    "    'Uk': 'United Kingdom',\n",
    "    'European Union': pd.NA,\n",
    "    'Eu': pd.NA,\n",
    "    'England': 'United Kingdom',\n",
    "    'Great Britain': 'United Kingdom',\n",
    "    'España': 'Spain',\n",
    "    'España\"': 'Spain',\n",
    "    'Spain\"': 'Spain',\n",
    "    'France\"': 'France',\n",
    "    'Portugal\"': 'Portugal',\n",
    "    'Switzerland\"': 'Switzerland',\n",
    "    'New Zealand\"': 'New Zealand',\n",
    "    'Netherlands\"': 'Netherlands',\n",
    "    'Norway\"': 'Norway',\n",
    "    'America': 'United States',\n",
    "    'Ysa': 'United States',\n",
    "    'Universe': pd.NA,\n",
    "    '\"N/A\"': pd.NA,\n",
    "    \"NA\": pd.NA,\n",
    "    '\\\\\"N/A\\\\\"\"': pd.NA,\n",
    "    'Alachua': pd.NA,\n",
    "    'Polk': pd.NA,\n",
    "    'Quit': pd.NA,\n",
    "    'Pender': pd.NA,\n",
    "    'Urugua': 'Uruguay',\n",
    "    'Cherokee': pd.NA,\n",
    "    'Galiza': pd.NA,\n",
    "    'Catalunya': 'Spain',\n",
    "    'Catalunya Spain': 'Spain',\n",
    "    'Deutschland': 'Germany',\n",
    "    'Euskal Herria': pd.NA,\n",
    "    'Catalonia': 'Spain',\n",
    "    'Catalonia Spain': 'Spain',\n",
    "    'Italia': 'Italy',\n",
    "    'Canary Islands, Spain': 'Spain',\n",
    "    'Turkey\"': 'Turkey',\n",
    "    'EspaA': 'Spain',\n",
    "    'Basque Country': 'Spain',\n",
    "    'Lleida': 'Spain',\n",
    "    'Orense': 'Spain',\n",
    "    'Madrid': 'Spain',\n",
    "    'Vicenza': 'Italy',\n",
    "    'Lombardia': 'Italy',\n",
    "    'La Belgique': 'Belgium',\n",
    "    'La France': 'France',\n",
    "    'La Suisse': 'Switzerland',\n",
    "    'Brasil': 'Brazil',\n",
    "    'PRChina': 'China',\n",
    "    'GuineaBissau': 'Guinea-Bissau',\n",
    "    'The Netherlands': 'Netherlands',\n",
    "    'Holland': 'Netherlands',\n",
    "    'Tobago': 'Trinidad and Tobago',\n",
    "    'Guernsey': 'United Kingdom',\n",
    "    'Jersey': 'United Kingdom',\n",
    "    'Alderney': 'United Kingdom',\n",
    "    'Wales': 'United Kingdom',\n",
    "    'Scotland': 'United Kingdom',\n",
    "    'Ouranos': pd.NA,\n",
    "    'Far Away': pd.NA,\n",
    "    'Galiza Neghra': pd.NA,\n",
    "    'Burlington': pd.NA,\n",
    "    'Hernando': pd.NA,\n",
    "    '': pd.NA,\n",
    "    'US Virgin Islands': 'United States',\n",
    "    'Bermuda': 'United Kingdom',\n",
    "    'Cayman Islands': 'United Kingdom',\n",
    "    'Guam': 'United States',\n",
    "    'Macau': 'China',\n",
    "    'Puerto Rico': 'United States',\n",
    "    'Yugoslavia': pd.NA, \n",
    "    'Antarctica': pd.NA,\n",
    "    'Usa  Canada': pd.NA,\n",
    "    'Trinidad and Tobago': 'Trinidad And Tobago'  # unify casing if needed\n",
    "}\n",
    "\n",
    "\n",
    "# Apply fixes\n",
    "users_df_cleaned['Country'] = users_df_cleaned['Country'].replace(country_fixes)\n",
    "users_df_cleaned = users_df_cleaned.dropna(subset=['Country'])\n",
    "users_df_cleaned = users_df_cleaned.reset_index(drop=True)\n",
    "users_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5b8554c-6a71-4fb4-95f8-b768b883326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_bad_encoding(city):\n",
    "    if not isinstance(city, str):\n",
    "        return False\n",
    "    return bool(re.search(r'[ÃãÂ±ðÃ©Ã¨ÃªÃ¼Ã¶Ã¤ÃŸÃªÃ¢Ã«ÃŽÃ®Ã»ÃƒÃ€Ã¦]', city))\n",
    "\n",
    "users_df_cleaned['City'] = users_df_cleaned['City'].apply(\n",
    "    lambda x: None if is_bad_encoding(x) else x\n",
    ")\n",
    "\n",
    "\n",
    "# Убираем акценты, диакритику и переводим всё к ASCII\n",
    "users_df_cleaned['City'] = users_df_cleaned['City'].apply(\n",
    "    lambda x: unidecode(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "city_fixes = {\n",
    "    'Slfjk': pd.NA,\n",
    "    'Mei Foo': pd.NA,\n",
    "    'Cmch': pd.NA,\n",
    "    'Moeville': pd.NA,\n",
    "    'Tuilla De Langreo': pd.NA,\n",
    "    'Island Lake Il': pd.NA,\n",
    "    'Ny': 'New York City',\n",
    "    'Nyc': 'New York City',\n",
    "    'Sf': 'San Francisco',\n",
    "    'La': 'Los Angeles',\n",
    "    'Ft Sill': 'Fort Sill',\n",
    "    'Ftstewart': 'Fort Stewart',\n",
    "    'Wien': 'Vienna',\n",
    "    'Roma': 'Rome',\n",
    "    'Munchen': 'Munich',\n",
    "    'Muenchen': 'Munich',\n",
    "    'Roma': 'Rome',\n",
    "    'Milano': 'Milan',\n",
    "    'Londres': 'London',\n",
    "    'Lisboa': 'Lisbon',\n",
    "    'Koeln': 'Cologne',\n",
    "    'Wien': 'Vienna',\n",
    "    'Frankfurt Am Main': 'Frankfurt',\n",
    "    'Stuttgart - West': 'Stuttgart',\n",
    "    'Ccp': pd.NA,\n",
    "    'Koln': 'Cologne',\n",
    "    'Sao Paolo': 'Sao Paulo',\n",
    "    'Distrito Federal': 'Mexico City',\n",
    "    'Albq': 'Albuquerque',\n",
    "    'Cambridge, UK': 'Cambridge',\n",
    "    'Saint Louis': 'St. Louis',\n",
    "    'Saint Paul': 'St. Paul',\n",
    "    'Newcastle Upon Tyne': 'Newcastle',\n",
    "    'Winston Salem': 'Winston-Salem',\n",
    "    'New York': 'New York City',\n",
    "    'Washington Dc': 'Washington',\n",
    "    'Newcastle Upon  Tyne': 'Newcastle Upon Tyne'\n",
    "}\n",
    "\n",
    "# Стандартизируем: убираем пробелы, приводим к Title Case\n",
    "users_df_cleaned['City'] = (\n",
    "    users_df_cleaned['City']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.title()\n",
    ")\n",
    "\n",
    "users_df_cleaned['City'] = users_df_cleaned['City'].str.split(',').str[0]\n",
    "users_df_cleaned['City'] = users_df_cleaned['City'].replace(city_fixes)\n",
    "users_df_cleaned = users_df_cleaned.dropna(subset=['City'])\n",
    "users_df_cleaned = users_df_cleaned[users_df_cleaned['City'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfc6933c-8ecd-4191-8c37-14fef65abd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>Stockton</td>\n",
       "      <td>California</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>Albacete</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>Langhorne</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>Quebec</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "      <td>Viterbo</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID  Age       City         State        Country\n",
       "0        2   18   Stockton    California  United States\n",
       "1       10   26   Albacete     Wisconsin          Spain\n",
       "2       20   19  Langhorne  Pennsylvania  United States\n",
       "3       36   24   Montreal        Quebec         Canada\n",
       "4       38   34    Viterbo         Lazio          Italy"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "574c49ec-378c-49d5-83bf-80aa9261dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.rename(columns={'User-ID': 'User_ID', 'Book-Rating': 'Book_Rating'}, inplace=True)\n",
    "users_df_cleaned.rename(columns={'User-ID': 'User_ID'}, inplace=True)\n",
    "books_df_cleaned.rename(columns={'Book-Title': 'Book_Title', 'Book-Author': 'Book_Author', 'Year-Of-Publication': 'Year_Of_Publication'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ef7af94-344a-4e3c-8983-8cc59c7f20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_cleaned.to_csv(\"books_cleaned_2.csv\", index=False)\n",
    "ratings_df.to_csv(\"ratings_cleaned_2.csv\", index=False)\n",
    "users_df_cleaned.to_csv(\"users_cleaned_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418683a-a4ff-4912-9bb7-950030096bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2f071-cca7-422f-b3b5-57e0a84505ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd830b-7d6f-41ec-843c-4692e9d44c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b495a-7e78-4e1d-8cb3-5514770b6f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41777288-5d5c-431f-8dbf-dddf49a5c555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea47dde2-1c10-416a-8ab6-96ff63eca4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9384de-3b8b-4e88-a483-6285e0d10fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22a4e9-51f0-466b-9b85-e39555ab9f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088bde96-10f9-4f03-85b9-c1683d666b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb64905-8402-4176-8c02-16126a7d314d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48978d-41bc-4bc0-b074-6a7fd628a290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be4ac6-94a5-4994-b4d3-c032c45aa524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848abe3-afc9-4c61-9577-5d98ea45ae8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
